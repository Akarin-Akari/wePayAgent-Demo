#!/usr/bin/env python3
"""
RAGå®æˆ˜Demo - å¾®ä¿¡æ”¯ä»˜æ™ºèƒ½å®¢æœ
=================================
è¿™æ˜¯ä¸€ä¸ªå®Œæ•´å¯è¿è¡Œçš„RAGç¤ºä¾‹ï¼Œæ¨¡æ‹Ÿå¾®ä¿¡æ”¯ä»˜å®¢æœåœºæ™¯ã€‚

è¿è¡Œæ–¹å¼ï¼š
1. ä½¿ç”¨OpenAI APIï¼ˆéœ€è¦API Keyï¼‰
   set OPENAI_API_KEY=sk-xxx
   python rag_demo.py

2. ä½¿ç”¨æœ¬åœ°æ¨¡æ‹Ÿæ¨¡å¼ï¼ˆæ— éœ€API Keyï¼Œç”¨äºç†è§£æµç¨‹ï¼‰
   python rag_demo.py --mock

ä½œè€…ï¼šé¢è¯•å‡†å¤‡ç”¨Demo
"""

import os
import sys
import argparse
from pathlib import Path

# ============================================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šæ–‡æ¡£åŠ è½½ä¸åˆ†å—
# ============================================================

def load_documents(knowledge_dir: str) -> list[str]:
    """
    åŠ è½½çŸ¥è¯†åº“æ–‡æ¡£
    
    Args:
        knowledge_dir: çŸ¥è¯†åº“ç›®å½•è·¯å¾„
    
    Returns:
        æ–‡æ¡£å†…å®¹åˆ—è¡¨
    """
    documents = []
    knowledge_path = Path(knowledge_dir)
    
    for file_path in knowledge_path.glob("*.txt"):
        print(f"ğŸ“„ åŠ è½½æ–‡æ¡£: {file_path.name}")
        with open(file_path, "r", encoding="utf-8") as f:
            documents.append(f.read())
    
    return documents


def chunk_documents(documents: list[str], chunk_size: int = 500, overlap: int = 100) -> list[str]:
    """
    ä½¿ç”¨ LangChain çš„ RecursiveCharacterTextSplitter è¿›è¡Œæ™ºèƒ½åˆ†å—
    é€‚åˆä¸­æ–‡ç¯å¢ƒï¼ˆä¼˜å…ˆæŒ‰æ®µè½ã€å¥å­ã€æ ‡ç‚¹åˆ†éš”ï¼‰
    """
    try:
        from langchain.text_splitter import RecursiveCharacterTextSplitter
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=overlap,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", " ", ""]
        )
        print("ğŸ”ª ä½¿ç”¨ RecursiveCharacterTextSplitter è¿›è¡Œåˆ†å—...")
        return text_splitter.split_text("\n\n".join(documents))
    except ImportError:
        print("âš ï¸ æœªæ‰¾åˆ° langchainï¼Œé™çº§ä½¿ç”¨ç®€æ˜“åˆ†å—...")
        # Fallback implementation
        chunks = []
        for doc in documents:
            current_chunk = ""
            for line in doc.split("\n"):
                if len(current_chunk) + len(line) < chunk_size:
                    current_chunk += line + "\n"
                else:
                    chunks.append(current_chunk)
                    current_chunk = line + "\n"
            if current_chunk: chunks.append(current_chunk)
        return chunks

# ============================================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šEmbeddingï¼ˆå‘é‡åŒ–ï¼‰
# ============================================================

class MockEmbedding:
    """
    æ¨¡æ‹ŸEmbeddingç±»ï¼ˆç”¨äºæ— API Keyæ—¶æ¼”ç¤ºæµç¨‹ï¼‰
    ä½¿ç”¨ç®€å•çš„è¯é¢‘ç»Ÿè®¡æ¨¡æ‹Ÿå‘é‡
    """
    def __init__(self):
        self.vocab = {}
        self.dim = 100
    
    def embed(self, text: str) -> list[float]:
        """ç®€å•çš„è¯è¢‹æ¨¡å‹æ¨¡æ‹ŸEmbedding"""
        import hashlib
        # ç”¨hashæ¨¡æ‹Ÿå‘é‡ï¼ˆä»…ç”¨äºæ¼”ç¤ºï¼Œå®é™…ä¸èƒ½è¿™æ ·åšï¼‰
        hash_bytes = hashlib.md5(text.encode()).digest()
        return [b / 255.0 for b in hash_bytes[:self.dim]]


class OpenAIEmbedding:
    """
    OpenAI Embeddingå°è£…
    """
    def __init__(self, model: str = "text-embedding-3-small"):
        from openai import OpenAI
        self.client = OpenAI()
        self.model = model
    
    def embed(self, text: str) -> list[float]:
        """è°ƒç”¨OpenAI Embedding API"""
        response = self.client.embeddings.create(
            model=self.model,
            input=text
        )
        return response.data[0].embedding

# ============================================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šå‘é‡å­˜å‚¨ä¸æ£€ç´¢
# ============================================================

class SimpleVectorStore:
    """
    ç®€å•å‘é‡å­˜å‚¨ï¼ˆæ”¯æŒ å‘é‡æ£€ç´¢ å’Œ å…³é”®è¯æ£€ç´¢ï¼‰
    """
    def __init__(self):
        self.documents = []  # åŸå§‹æ–‡æœ¬
        self.embeddings = []  # å¯¹åº”çš„å‘é‡
    
    def add(self, text: str, embedding: list[float]):
        """æ·»åŠ æ–‡æ¡£å’Œå‘é‡"""
        self.documents.append(text)
        self.embeddings.append(embedding)
    
    def search(self, query: str, query_embedding: list[float], top_k: int = 3) -> list[tuple[str, float]]:
        """
        æ£€ç´¢ç­–ç•¥ï¼šæ··åˆæ£€ç´¢ (Vector + Keyword)
        å¦‚æœæ˜¯ Real Embedding (bge-m3)ï¼Œå‘é‡ç›¸ä¼¼åº¦æƒé‡é«˜
        å¦‚æœæ˜¯ Mock/Failï¼Œå…³é”®è¯æƒé‡é«˜
        """
        import numpy as np
        
        scores = []
        query_tokens = set(query.lower())
        
        # å‘é‡æ£€ç´¢ç®—æ³• (Cosine Similarity)
        use_vector = True
        try:
             # å¦‚æœå‘é‡ç»´åº¦å¾ˆå°æˆ–è€…å…¨æ˜¯0ï¼Œæˆ–è€…æ˜¯Mockçš„é•¿åº¦(100)ï¼Œå¯èƒ½è´¨é‡ä¸é«˜
             # bge-m3 ç»´åº¦é€šå¸¸æ˜¯ 1024
             if len(query_embedding) < 128 or all(x == 0 for x in query_embedding):
                 use_vector = False
        except:
            use_vector = False

        for i, doc_text in enumerate(self.documents):
            # 1. å…³é”®è¯å¾—åˆ† (Jaccard)
            doc_tokens = set(doc_text.lower())
            intersection = query_tokens.intersection(doc_tokens)
            union = query_tokens.union(doc_tokens)
            jaccard_score = len(intersection) / len(union) if union else 0
            
            # ç‰¹å®šå…³é”®è¯åŠ å¼º
            keyword_hits = 0
            if "T+1" in query and "T+1" in doc_text: keyword_hits += 1.0
            
            # 2. å‘é‡å¾—åˆ†
            vector_score = 0.0
            if use_vector:
                doc_vec = np.array(self.embeddings[i])
                q_vec = np.array(query_embedding)
                norm_q = np.linalg.norm(q_vec)
                norm_d = np.linalg.norm(doc_vec)
                if norm_q > 0 and norm_d > 0:
                    vector_score = np.dot(q_vec, doc_vec) / (norm_q * norm_d)
            
            # ç»¼åˆæ‰“åˆ†ç­–ç•¥
            if use_vector:
                # çœŸå®Embeddingåœºæ™¯ï¼š70% å‘é‡ + 30% å…³é”®è¯ (åŠ å¼ºé²æ£’æ€§)
                final_score = vector_score * 0.7 + jaccard_score * 0.3 + keyword_hits * 0.2
            else:
                # Mock/Fallbackåœºæ™¯ï¼šçº¯å…³é”®è¯
                final_score = jaccard_score * 0.8 + keyword_hits * 0.5
            
            scores.append((i, final_score))
        
        scores.sort(key=lambda x: x[1], reverse=True)
        return [(self.documents[i], s) for i, s in scores[:top_k]]

# ============================================================
# ç¬¬å››éƒ¨åˆ†ï¼šLLMç”Ÿæˆ
# ============================================================

class MockLLM:
    """æ¨¡æ‹ŸLLMï¼ˆç”¨äºæ— API Keyæ—¶æ¼”ç¤ºï¼‰"""
    def generate(self, prompt: str) -> str:
        return f"[æ¨¡æ‹Ÿå›ç­”] æ ¹æ®æä¾›çš„èµ„æ–™ï¼Œæˆ‘æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚ï¼ˆè¿™æ˜¯æ¨¡æ‹Ÿè¾“å‡ºï¼Œå®é™…ä¼šè°ƒç”¨LLM APIï¼‰"


class OpenAILLM:
    """OpenAI LLMå°è£…"""
    def __init__(self, model: str = "gpt-3.5-turbo"):
        from openai import OpenAI
        self.client = OpenAI()
        self.model = model
    
    def generate(self, prompt: str) -> str:
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7
        )
        return response.choices[0].message.content


class OllamaEmbedding:
    """Ollama Embeddingå°è£…"""
    def __init__(self, model: str = "bge-m3"):
        import requests
        self.model = model
        self.base_url = "http://localhost:11434"
    
    def embed(self, text: str) -> list[float]:
        """è°ƒç”¨Ollama Embedding API - å¤±è´¥åˆ™è¿”å›Mockå‘é‡"""
        import requests
        import hashlib
        try:
            response = requests.post(
                f"{self.base_url}/api/embeddings",
                json={"model": self.model, "prompt": text},
                timeout=30
            )
            if response.status_code == 200:
                emb = response.json().get("embedding")
                if emb: return emb
            else:
                print(f"âš ï¸ Embedding API Error: {response.text}")
        except Exception as e:
            print(f"âš ï¸ Embedding Exception: {e}")
        
        # Fallback to Mock (MD5) if API fails
        # This works with our new Keyword Search logic in SimpleVectorStore
        hash_bytes = hashlib.md5(text.encode()).digest()
        # Create a dummy 1024-dim vector to be safe/compatible if needed, but SimpleVectorStore handles short vectors as Mock
        return [b / 255.0 for b in hash_bytes[:100]]


class OllamaLLM:
    """Ollama LLMå°è£…"""
    def __init__(self, model: str = "qwen3:4b"):
        self.model = model
        self.base_url = "http://localhost:11434"
    
    def generate(self, prompt: str) -> str:
        """è°ƒç”¨Ollama Chat API (æµå¼ç‰ˆ)"""
        import requests
        import json
        
        # å‘èµ·æµå¼è¯·æ±‚
        response = requests.post(
            f"{self.base_url}/api/generate",
            json={
                "model": self.model,
                "prompt": prompt,
                "stream": True,  # å¼€å¯æµå¼ä»¥è·å–æ€è€ƒè¿‡ç¨‹
                "options": {
                    "num_ctx": 4096,
                    "num_predict": 2048,  # è°ƒå¤§ä»¥ä¾¿å®¹çº³å®Œæ•´çš„æ€è€ƒ+å›ç­”
                    "temperature": 0.6,
                }
            },
            stream=True
        )
        
        full_response = ""
        print(f"\n{'='*20} æ¨¡å‹æ€è€ƒä¸å›ç­” {'='*20}\n")
        
        for line in response.iter_lines():
            if line:
                try:
                    json_obj = json.loads(line.decode('utf-8'))
                    chunk = json_obj.get("response", "")
                    if chunk:
                        print(chunk, end="", flush=True)
                        full_response += chunk
                except:
                    continue
                    
        print(f"\n\n{'='*55}\n")
        return full_response

# ============================================================
# ç¬¬äº”éƒ¨åˆ†ï¼šRAG Pipeline
# ============================================================

class RAGPipeline:
    # ... (init and index_documents remain same) ...
    def __init__(self, embedding_model, llm, vector_store):
        self.embedding = embedding_model
        self.llm = llm
        self.vector_store = vector_store
    
    def index_documents(self, chunks: list[str]):
        """ç´¢å¼•æ–‡æ¡£ï¼ˆç¦»çº¿é˜¶æ®µï¼‰"""
        print("\nğŸ”§ å¼€å§‹ç´¢å¼•æ–‡æ¡£...")
        for i, chunk in enumerate(chunks):
            emb = self.embedding.embed(chunk)
            self.vector_store.add(chunk, emb)
            if (i + 1) % 5 == 0:
                print(f"   å·²ç´¢å¼• {i + 1}/{len(chunks)} ä¸ªå—")
        print("âœ… ç´¢å¼•å®Œæˆ!")

    def query(self, question: str, top_k: int = 3) -> str:
        """
        å›ç­”é—®é¢˜ï¼ˆåœ¨çº¿é˜¶æ®µï¼‰
        """
        print(f"\nâ“ é—®é¢˜: {question}")
        
        # Step 1: é—®é¢˜å‘é‡åŒ–
        print("   ğŸ“ Step 1: é—®é¢˜Embedding...")
        query_emb = self.embedding.embed(question)
        
        # Step 2: æ£€ç´¢ç›¸å…³æ–‡æ¡£
        print("   ğŸ“ Step 2: æ£€ç´¢ç›¸å…³æ–‡æ¡£ (æ··åˆæ£€ç´¢)...")
        # ä¿®æ”¹ search ç­¾åï¼Œä¼ å…¥ question æ–‡æœ¬ä»¥ä¾¿è¿›è¡Œå…³é”®è¯æ£€ç´¢
        results = self.vector_store.search(question, query_emb, top_k=top_k)
        
        print(f"   ğŸ“ æ£€ç´¢åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£å—:")
        for i, (doc, score) in enumerate(results):
            preview = doc[:50].replace("\n", " ") + "..."
            print(f"      [{i+1}] å¾—åˆ†={score:.3f} | {preview}")
        
        # Step 3: æ„å»ºPrompt
        print("   ğŸ“ Step 3: æ„å»ºPrompt (å®Œæ•´ä¸Šä¸‹æ–‡)...")
        # ç§»é™¤ [:500] æˆªæ–­ï¼å…è®¸å…¨éƒ¨æ£€ç´¢ç»“æœè¿›å…¥ Prompt
        context = "\n---\n".join([doc for doc, _ in results])
        
        prompt = f"""æ ¹æ®ä»¥ä¸‹å‚è€ƒèµ„æ–™å›ç­”é—®é¢˜ã€‚å¦‚æœèµ„æ–™ä¸­æ²¡æœ‰æåˆ°ï¼Œè¯·è¯´ä¸çŸ¥é“ã€‚

ã€å‚è€ƒèµ„æ–™ã€‘
{context}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{question}

ã€å›ç­”ã€‘"""
        
        # Step 4: LLMç”Ÿæˆ
        print("   ğŸ“ Step 4: LLMç”Ÿæˆå›ç­”...")
        answer = self.llm.generate(prompt)
        
        return answer

# ============================================================
# ä¸»ç¨‹åº
# ============================================================

def main():
    parser = argparse.ArgumentParser(description="RAG Demo - å¾®ä¿¡æ”¯ä»˜æ™ºèƒ½å®¢æœ")
    parser.add_argument("--mock", action="store_true", help="ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼ï¼ˆæ— éœ€API Keyï¼‰")
    parser.add_argument("--ollama", action="store_true", help="ä½¿ç”¨Ollamaæœ¬åœ°æ¨¡å‹")
    parser.add_argument("--model", type=str, default="qwen3:4b", help="Ollamaæ¨¡å‹åç§°")
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸ¤– RAGå®æˆ˜Demo - å¾®ä¿¡æ”¯ä»˜æ™ºèƒ½å®¢æœ")
    print("=" * 60)
    
    # ç¡®å®šçŸ¥è¯†åº“è·¯å¾„
    script_dir = Path(__file__).parent
    knowledge_dir = script_dir / "knowledge_base"
    
    if not knowledge_dir.exists():
        print(f"âŒ çŸ¥è¯†åº“ç›®å½•ä¸å­˜åœ¨: {knowledge_dir}")
        sys.exit(1)
    
    # åˆå§‹åŒ–ç»„ä»¶
    if args.ollama:
        print(f"\nâš¡ è¿è¡Œæ¨¡å¼: Ollamaæœ¬åœ°æ¨¡å‹ ({args.model})")
        # Ollama çš„ embedding å¯èƒ½ä¸æ”¯æŒæ‰€æœ‰æ¨¡å‹ï¼Œç”¨ Mock æ›¿ä»£
        embedding = MockEmbedding()
        llm = OllamaLLM(model=args.model)
    elif args.mock:
        print("\nâš¡ è¿è¡Œæ¨¡å¼: æ¨¡æ‹Ÿæ¨¡å¼ï¼ˆä¸è°ƒç”¨çœŸå®APIï¼‰")
        embedding = MockEmbedding()
        llm = MockLLM()
    else:
        if not os.environ.get("OPENAI_API_KEY"):
            print("\nâš ï¸  è­¦å‘Š: æœªè®¾ç½®OPENAI_API_KEYï¼Œåˆ‡æ¢åˆ°æ¨¡æ‹Ÿæ¨¡å¼")
            print("   å¦‚éœ€ä½¿ç”¨çœŸå®APIï¼Œè¯·è¿è¡Œ: set OPENAI_API_KEY=sk-xxx")
            embedding = MockEmbedding()
            llm = MockLLM()
        else:
            print("\nâš¡ è¿è¡Œæ¨¡å¼: OpenAI APIæ¨¡å¼")
            embedding = OpenAIEmbedding()
            llm = OpenAILLM()
    
    vector_store = SimpleVectorStore()
    
    # åˆ›å»ºRAG Pipeline
    rag = RAGPipeline(embedding, llm, vector_store)
    
    # ===== ç¦»çº¿é˜¶æ®µï¼šç´¢å¼•æ–‡æ¡£ =====
    print("\n" + "=" * 60)
    print("ğŸ“š ç¦»çº¿é˜¶æ®µï¼šåŠ è½½å’Œç´¢å¼•çŸ¥è¯†åº“")
    print("=" * 60)
    
    documents = load_documents(str(knowledge_dir))
    chunks = chunk_documents(documents)
    rag.index_documents(chunks)
    
    # ===== åœ¨çº¿é˜¶æ®µï¼šé—®ç­” =====
    print("\n" + "=" * 60)
    print("ğŸ’¬ åœ¨çº¿é˜¶æ®µï¼šæ™ºèƒ½é—®ç­”")
    print("=" * 60)
    
    # é¢„è®¾é—®é¢˜æ¼”ç¤º
    demo_questions = [
        "å¾®ä¿¡æ”¯ä»˜çš„ç»“ç®—å‘¨æœŸæ˜¯å¤šä¹…ï¼Ÿ",
        "å•†æˆ·è´¹ç‡æ˜¯å¤šå°‘ï¼Ÿ",
        "å¦‚ä½•æˆä¸ºå¾®ä¿¡æ”¯ä»˜æœåŠ¡å•†ï¼Ÿ",
    ]
    
    for q in demo_questions:
        answer = rag.query(q)
        print(f"\nğŸ’¡ å›ç­”:\n{answer}")
        print("\n" + "-" * 60)
    
    # äº¤äº’å¼é—®ç­”
    print("\n" + "=" * 60)
    print("ğŸ¤ è¿›å…¥äº¤äº’æ¨¡å¼ï¼ˆè¾“å…¥ quit é€€å‡ºï¼‰")
    print("=" * 60)
    
    while True:
        try:
            user_input = input("\nğŸ‘¤ è¯·è¾“å…¥é—®é¢˜: ").strip()
            if user_input.lower() in ["quit", "exit", "q"]:
                print("ğŸ‘‹ å†è§ï¼")
                break
            if not user_input:
                continue
            
            answer = rag.query(user_input)
            print(f"\nğŸ’¡ å›ç­”:\n{answer}")
            
        except KeyboardInterrupt:
            print("\nğŸ‘‹ å†è§ï¼")
            break


if __name__ == "__main__":
    main()
