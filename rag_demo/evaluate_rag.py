#!/usr/bin/env python3
"""
RAG è¯„æµ‹è„šæœ¬
=================================
ä½¿ç”¨ LLM-as-a-Judge æ¨¡å¼å¯¹ RAG ç³»ç»Ÿè¿›è¡Œè‡ªåŠ¨åŒ–è¯„æµ‹ã€‚

è¯„æµ‹ç»´åº¦ï¼š
1. å…³é”®è¯è¦†ç›–ç‡ (Keyword Hit Rate) - ç¡¬æŒ‡æ ‡
2. LLM è¯­ä¹‰æ‰“åˆ† (Semantic Score) - è½¯æŒ‡æ ‡ (0-10åˆ†)
"""

import json
import sys
import os
import argparse
from pathlib import Path
from rag_demo import (
    load_documents, 
    chunk_documents, 
    OllamaEmbedding, 
    OllamaLLM, 
    SimpleVectorStore, 
    RAGPipeline
)

def evaluate_answer(judge_llm, question, expected, actual) -> dict:
    """ä½¿ç”¨ LLM å¯¹é—®ç­”è´¨é‡è¿›è¡Œæ‰“åˆ†"""
    prompt = f"""ä½œä¸ºå…¬å¹³çš„è¯„æµ‹å‘˜ï¼Œè¯·è¯„ä¼°ä»¥ä¸‹AIå›ç­”çš„è´¨é‡ã€‚

ã€ç”¨æˆ·é—®é¢˜ã€‘{question}
ã€æ ‡å‡†ç­”æ¡ˆã€‘{expected}
ã€AI å›ç­”ã€‘{actual}

è¯·æ ¹æ®AIå›ç­”æ˜¯å¦å‡†ç¡®åŒ…å«äº†æ ‡å‡†ç­”æ¡ˆçš„æ ¸å¿ƒä¿¡æ¯è¿›è¡Œæ‰“åˆ†ï¼ˆ0-10åˆ†ï¼‰ã€‚
0åˆ†ï¼šå®Œå…¨é”™è¯¯æˆ–æœªå›ç­”
5åˆ†ï¼šéƒ¨åˆ†æ­£ç¡®ï¼Œä½†æœ‰é—æ¼
10åˆ†ï¼šå®Œå…¨æ­£ç¡®ä¸”è¡¨è¿°æ¸…æ™°

è¯·ä»…è¿”å› JSON æ ¼å¼ç»“æœï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
    "score": 8,
    "reason": "å›ç­”å‡†ç¡®ï¼Œè¦†ç›–äº†æ ¸å¿ƒç‚¹"
}}
"""
    try:
        # å¼ºåˆ¶ LLM è¾“å‡º JSON
        response_str = judge_llm.generate(prompt + "\n\nè¯·åªè¾“å‡ºJSONæ ¼å¼ (ä¾‹å¦‚: {\"score\": 8, \"reason\": \"...\"})ï¼Œä¸è¦åŒ…å«Markdownæˆ–å…¶ä»–æ–‡å­—ã€‚")
        
        # å°è¯•æ¸…ç† markdown
        clean_text = response_str.replace("```json", "").replace("```", "").strip()
        
        # å°è¯•ç›´æ¥è§£æ
        try:
            return json.loads(clean_text)
        except json.JSONDecodeError:
            pass

        # å¦‚æœ JSON è§£æå¤±è´¥ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–åˆ†æ•°
        import re
        score_match = re.search(r'"score":\s*(\d+)', clean_text)
        if not score_match:
             score_match = re.search(r'score:\s*(\d+)', clean_text)
        
        score = int(score_match.group(1)) if score_match else 0
        
        # æå– Reason (ç®€å•å¤„ç†)
        reason = "Parsed from text"
        if "reason" in clean_text:
            parts = clean_text.split("reason")
            if len(parts) > 1:
                reason = parts[1].strip().strip('":,').strip()
        
        return {"score": score, "reason": reason}

    except Exception as e:
        print(f"âš ï¸ è¯„æµ‹æ‰“åˆ†å¤±è´¥: {e} | Raw: {response_str[:50]}...")
        return {"score": 5, "reason": "Evaluator Parsing Failed (Default 5)"}

def main():
    # 1. åˆå§‹åŒ– Pipeline
    print("ğŸš€ åˆå§‹åŒ– RAG Pipeline...")
    knowledge_dir = "./knowledge_base"
    docs = load_documents(knowledge_dir)
    chunks = chunk_documents(docs)
    
    # å¼ºåˆ¶åšä¸€æ¬¡ç¦»çº¿ç´¢å¼•
    embed_model = OllamaEmbedding(model="bge-m3")
    llm = OllamaLLM(model="qwen3:4b")
    vector_store = SimpleVectorStore()
    
    pipeline = RAGPipeline(embed_model, llm, vector_store)
    pipeline.index_documents(chunks)
    
    # 2. åŠ è½½æµ‹è¯•é›†
    data_path = Path("./data/benchmark_qa.json")
    if not data_path.exists():
        print(f"âŒ æœªæ‰¾åˆ°æµ‹è¯•é›†: {data_path}")
        return
        
    with open(data_path, "r", encoding="utf-8") as f:
        test_cases = json.load(f)
    
    print(f"\nğŸ§ª å¼€å§‹è¯„æµ‹ (å…± {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹)...\n")
    
    total_score = 0
    total_keyword_hit = 0
    total_expected_keywords = 0
    
    results = []
    
    for i, case in enumerate(test_cases):
        q = case["question"]
        expected = case["expected_answer"]
        keywords = case.get("keywords", [])
        
        print(f"[{i+1}/{len(test_cases)}] æé—®: {q}")
        
        # è¿è¡Œ RAG
        actual = pipeline.query(q, top_k=3)
        print(f"   ğŸ¤– å›ç­”: {actual.strip()[:60]}...")
        
        # 1. å…³é”®è¯è¯„æµ‹
        hits = sum(1 for k in keywords if k in actual)
        keyword_rate = hits / len(keywords) if keywords else 1.0
        total_keyword_hit += hits
        total_expected_keywords += len(keywords)
        
        # 2. LLM æ‰“åˆ†
        eval_result = evaluate_answer(llm, q, expected, actual)
        score = eval_result.get("score", 0)
        total_score += score
        
        print(f"   ğŸ“Š è¯„æµ‹: å¾—åˆ†={score}/10 | å…³é”®è¯å‘½ä¸­={hits}/{len(keywords)}")
        print(f"   ğŸ’¡ ç†ç”±: {eval_result.get('reason')}\n")
        
        results.append({
            "question": q,
            "actual": actual,
            "score": score,
            "keyword_rate": keyword_rate
        })

    # 3. æ±‡æ€»æŠ¥å‘Š
    avg_score = total_score / len(test_cases)
    avg_keyword_rate = (total_keyword_hit / total_expected_keywords * 100) if total_expected_keywords else 0
    
    print("="*40)
    print("       RAG è¯„æµ‹æŠ¥å‘Š Result")
    print("="*40)
    print(f"Tests: {len(test_cases)}")
    print(f"Avg Semantic Score: {avg_score:.1f} / 10")
    print(f"Keyword Coverage  : {avg_keyword_rate:.1f}%")
    print("="*40)

if __name__ == "__main__":
    main()
