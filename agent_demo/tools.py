#!/usr/bin/env python3
"""
Agent Tools - æ™ºèƒ½å®¢æœå·¥å…·é›†
================================
å°è£…RAGå’Œä¸šåŠ¡APIä¸ºAgentå¯è°ƒç”¨çš„æ ‡å‡†å·¥å…·
ä½¿ç”¨ ChromaDB æŒä¹…åŒ–å‘é‡å­˜å‚¨
"""

import os
import sys
from pathlib import Path

# å¯¼å…¥å‘é‡å­˜å‚¨æ¨¡å—
from vector_store import get_vector_store, ChromaVectorStore


# ============================================================
# Tool 1: çŸ¥è¯†åº“æ£€ç´¢ (RAG with ChromaDB)
# ============================================================

class KnowledgeBaseTool:
    """
    çŸ¥è¯†åº“æ£€ç´¢å·¥å…· - å°è£… RAG Pipeline
    
    ä½¿ç”¨ ChromaDB æŒä¹…åŒ–å‘é‡å­˜å‚¨ï¼š
    - é¦–æ¬¡è¿è¡Œè‡ªåŠ¨ç´¢å¼•çŸ¥è¯†åº“
    - åç»­è¿è¡Œç›´æ¥ä½¿ç”¨å·²æœ‰ç´¢å¼•
    - æ”¯æŒå¤§è§„æ¨¡çŸ¥è¯†åº“
    """
    name = "knowledge_search"
    description = "æŸ¥è¯¢å¾®ä¿¡æ”¯ä»˜å®˜æ–¹æ”¿ç­–ã€è´¹ç”¨æ ‡å‡†ã€æ“ä½œæŒ‡å—ç­‰çŸ¥è¯†ã€‚è¾“å…¥ï¼šç”¨æˆ·é—®é¢˜"
    
    def __init__(self, 
                 knowledge_dir: str = "../rag_demo/knowledge_base",
                 chroma_dir: str = "./chroma_db",
                 force_reindex: bool = False):
        """
        Args:
            knowledge_dir: çŸ¥è¯†åº“æ–‡æ¡£ç›®å½•
            chroma_dir: ChromaDB æŒä¹…åŒ–ç›®å½•
            force_reindex: å¼ºåˆ¶é‡æ–°ç´¢å¼•
        """
        self.knowledge_dir = Path(knowledge_dir)
        
        # ä½¿ç”¨ ChromaDB å‘é‡å­˜å‚¨
        self.vector_store = get_vector_store(
            use_chroma=True,
            persist_directory=chroma_dir,
            collection_name="wxpay_knowledge"
        )
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ç´¢å¼•
        if force_reindex or self.vector_store.count() == 0:
            self._index_documents()
        else:
            print(f"ğŸ“š [KnowledgeTool] ä½¿ç”¨å·²æœ‰ç´¢å¼• ({self.vector_store.count()} ä¸ªæ–‡æ¡£å—)")
    
    def _index_documents(self):
        """åŠ è½½å¹¶ç´¢å¼•çŸ¥è¯†åº“"""
        if not self.knowledge_dir.exists():
            print(f"âš ï¸ çŸ¥è¯†åº“ç›®å½•ä¸å­˜åœ¨: {self.knowledge_dir}")
            return
        
        documents = []
        for file_path in self.knowledge_dir.glob("*.txt"):
            print(f"ğŸ“„ [KnowledgeTool] åŠ è½½: {file_path.name}")
            with open(file_path, "r", encoding="utf-8") as f:
                documents.append(f.read())
        
        # æ™ºèƒ½åˆ†å—ï¼šæŒ‰ç« èŠ‚å’Œæ®µè½åˆ†å‰²
        chunks = self._smart_chunk(documents)
        
        print(f"ğŸ”§ [KnowledgeTool] æ­£åœ¨ç´¢å¼• {len(chunks)} ä¸ªæ–‡æ¡£å—åˆ° ChromaDB...")
        self.vector_store.add_documents(chunks)
        print("âœ… [KnowledgeTool] ç´¢å¼•å®Œæˆ!")
    
    def _smart_chunk(self, documents: list[str], chunk_size: int = 800) -> list[str]:
        """
        æ™ºèƒ½åˆ†å—ï¼šæŒ‰ç« èŠ‚æ ‡é¢˜åˆ†å‰²ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´æ€§
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            chunk_size: æœ€å¤§å—å¤§å°
        """
        chunks = []
        
        for doc in documents:
            lines = doc.split("\n")
            current_chunk = ""
            current_section = ""
            
            for line in lines:
                # æ£€æµ‹ç« èŠ‚æ ‡é¢˜
                if line.startswith("## ") or line.startswith("### "):
                    # ä¿å­˜ä¹‹å‰çš„å—
                    if current_chunk.strip():
                        chunks.append(current_chunk.strip())
                    # å¼€å§‹æ–°å—ï¼ŒåŒ…å«ç« èŠ‚æ ‡é¢˜
                    current_section = line
                    current_chunk = line + "\n"
                elif len(current_chunk) + len(line) < chunk_size:
                    current_chunk += line + "\n"
                else:
                    # å½“å‰å—æ»¡äº†ï¼Œä¿å­˜å¹¶å¼€å§‹æ–°å—
                    if current_chunk.strip():
                        chunks.append(current_chunk.strip())
                    # æ–°å—ä»¥ç« èŠ‚æ ‡é¢˜å¼€å¤´ï¼ˆä¿æŒä¸Šä¸‹æ–‡ï¼‰
                    if current_section:
                        current_chunk = current_section + "\n" + line + "\n"
                    else:
                        current_chunk = line + "\n"
            
            # ä¿å­˜æœ€åä¸€ä¸ªå—
            if current_chunk.strip():
                chunks.append(current_chunk.strip())
        
        return chunks
    
    def run(self, query: str) -> str:
        """æ‰§è¡ŒçŸ¥è¯†åº“æ£€ç´¢"""
        results = self.vector_store.search(query, top_k=3)
        
        if not results:
            return "æœªæ‰¾åˆ°ç›¸å…³çŸ¥è¯†ã€‚"
        
        # æ‹¼æ¥æ£€ç´¢ç»“æœ
        context_parts = []
        for i, (doc, score) in enumerate(results, 1):
            # æˆªå–å…³é”®éƒ¨åˆ†ï¼Œé¿å…è¿”å›å¤ªé•¿
            doc_preview = doc[:500] + "..." if len(doc) > 500 else doc
            context_parts.append(f"ã€{i}ã€‘(ç›¸å…³åº¦:{score:.2f})\n{doc_preview}")
        
        context = "\n---\n".join(context_parts)
        return f"ã€æ£€ç´¢åˆ°çš„çŸ¥è¯†ã€‘\n{context}"
    
    def reindex(self):
        """æ‰‹åŠ¨è§¦å‘é‡æ–°ç´¢å¼•"""
        print("ğŸ”„ [KnowledgeTool] æ¸…ç©ºå¹¶é‡æ–°ç´¢å¼•...")
        self.vector_store.clear()
        self._index_documents()


# ============================================================
# Tool 2: è®¢å•æŸ¥è¯¢ (æ¨¡æ‹Ÿä¸šåŠ¡API)
# ============================================================

class OrderQueryTool:
    """
    è®¢å•æŸ¥è¯¢å·¥å…· - æ¨¡æ‹Ÿä¸šåŠ¡ç³»ç»ŸAPI
    
    ç”¨äºå›ç­”ï¼šè®¢å•çŠ¶æ€ã€é€€æ¬¾è¿›åº¦ç­‰åŠ¨æ€ä¸šåŠ¡é—®é¢˜
    """
    name = "order_query"
    description = "æŸ¥è¯¢è®¢å•/é€€æ¬¾çŠ¶æ€ã€‚è¾“å…¥ï¼šè®¢å•å·æˆ–é€€æ¬¾å•å· (å¦‚ ORDER_1001, REF_999)"
    
    # æ¨¡æ‹Ÿè®¢å•æ•°æ®åº“
    MOCK_DB = {
        "ORDER_1001": {"status": "å·²å®Œæˆ", "amount": 99.00, "time": "2024-12-15 14:30", "refund": None},
        "ORDER_1002": {"status": "é€€æ¬¾ä¸­", "amount": 199.00, "time": "2024-12-10 09:00", "refund": "REF_2001"},
        "ORDER_1003": {"status": "å¾…æ”¯ä»˜", "amount": 59.00, "time": "2024-12-18 10:00", "refund": None},
        "REF_2001": {"status": "å¤„ç†ä¸­", "original_order": "ORDER_1002", "amount": 199.00, "eta": "1-3ä¸ªå·¥ä½œæ—¥"},
        "REF_2002": {"status": "å·²é€€æ¬¾", "original_order": "ORDER_999", "amount": 50.00, "completed": "2024-12-17"},
    }
    
    def run(self, order_id: str) -> str:
        """æŸ¥è¯¢è®¢å•/é€€æ¬¾çŠ¶æ€"""
        order_id = order_id.strip().upper()
        
        # å¦‚æœç”¨æˆ·åªè¾“å…¥äº†æ•°å­—ï¼Œå°è¯•è‡ªåŠ¨è¡¥å…¨å‰ç¼€
        if order_id.isdigit():
            possible_ids = [f"ORDER_{order_id}", f"REF_{order_id}"]
            for pid in possible_ids:
                if pid in self.MOCK_DB:
                    order_id = pid
                    break
        
        if order_id in self.MOCK_DB:
            record = self.MOCK_DB[order_id]
            if order_id.startswith("REF"):
                return f"ã€é€€æ¬¾å• {order_id}ã€‘çŠ¶æ€: {record['status']}, åŸè®¢å•: {record['original_order']}, é‡‘é¢: Â¥{record['amount']}"
            else:
                refund_info = f", å…³è”é€€æ¬¾: {record['refund']}" if record['refund'] else ""
                return f"ã€è®¢å• {order_id}ã€‘çŠ¶æ€: {record['status']}, é‡‘é¢: Â¥{record['amount']}, ä¸‹å•æ—¶é—´: {record['time']}{refund_info}"
        else:
            return f"æœªæ‰¾åˆ°è®¢å•å· {order_id}ï¼Œè¯·æ ¸å¯¹åé‡æ–°è¾“å…¥ã€‚"


# ============================================================
# å·¥å…·æ³¨å†Œè¡¨
# ============================================================

def get_all_tools() -> dict:
    """è¿”å›æ‰€æœ‰å¯ç”¨å·¥å…·"""
    return {
        "knowledge_search": KnowledgeBaseTool(),
        "order_query": OrderQueryTool(),
    }
